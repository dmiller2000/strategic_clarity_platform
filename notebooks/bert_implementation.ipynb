{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gH1kMeuKE0ie"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload your CSV file\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the verified data\n",
        "import pandas as pd\n",
        "df = pd.read_csv('processed_data.csv')\n",
        "print(f\"Loaded {len(df)} examples for BERT training\")"
      ],
      "metadata": {
        "id": "fUv7aSO4E_bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install transformers torch datasets accelerate\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "from transformers import (\n",
        "    DistilBertTokenizer,\n",
        "    DistilBertForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "print(\"‚úÖ Libraries installed and imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "id": "fa4qOFUpFbHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "# Read and fix the CSV formatting (same issue as before)\n",
        "with open('processed_data.csv', 'r', encoding='utf-8-sig') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Remove BOM and fix quote wrapping\n",
        "content = content.replace('Ôªø', '')\n",
        "lines = content.split('\\n')\n",
        "\n",
        "# Fix each line by removing outer quotes\n",
        "fixed_lines = []\n",
        "for line in lines:\n",
        "    if line.strip():\n",
        "        if line.startswith('\"') and line.endswith('\"'):\n",
        "            line = line[1:-1]\n",
        "        line = line.replace('\"\"', '\"')\n",
        "        fixed_lines.append(line)\n",
        "\n",
        "# Create properly formatted CSV\n",
        "fixed_content = '\\n'.join(fixed_lines)\n",
        "df = pd.read_csv(io.StringIO(fixed_content))\n",
        "\n",
        "print(f\"Loaded {len(df)} examples for BERT training\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"Patterns: {df['pattern'].value_counts()}\")\n",
        "\n",
        "# Prepare data for BERT\n",
        "X = df['text'].values\n",
        "y = df['pattern'].values\n",
        "\n",
        "# Create label mapping\n",
        "unique_labels = df['pattern'].unique()\n",
        "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "print(f\"Label mapping: {label2id}\")"
      ],
      "metadata": {
        "id": "9f1foX5WFoqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Create custom dataset class\n",
        "class WorkplaceDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Convert labels to numeric\n",
        "y_numeric = [label2id[label] for label in y]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_numeric, test_size=0.2, random_state=42, stratify=y_numeric\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = WorkplaceDataset(X_train, y_train, tokenizer)\n",
        "test_dataset = WorkplaceDataset(X_test, y_test, tokenizer)\n",
        "\n",
        "print(\"‚úÖ Datasets created successfully!\")"
      ],
      "metadata": {
        "id": "NmiWNvxMGuQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained DistilBERT model\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-uncased',\n",
        "    num_labels=len(label2id),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# Define training arguments (corrected parameter names)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./bert_results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\",  # Changed from evaluation_strategy\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_accuracy\",\n",
        "    greater_is_better=True,\n",
        "    dataloader_pin_memory=False,\n",
        "    report_to=\"none\", # Disable Weights & Biases logging\n",
        ")\n",
        "\n",
        "# Define metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, predictions, average='macro'\n",
        "    )\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Model and training setup complete!\")\n",
        "print(f\"Model loaded: DistilBERT with {len(label2id)} classes\")\n",
        "print(f\"Training epochs: 3\")\n",
        "print(f\"Batch size: 8 (CPU optimized)\")"
      ],
      "metadata": {
        "id": "kCxt69YBG8S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"üöÄ Starting BERT fine-tuning...\")\n",
        "print(\"This will take 10-15 minutes on CPU...\")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "print(\"‚úÖ Training completed!\")"
      ],
      "metadata": {
        "id": "uioupEaaHs0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation and results (RUN AFTER TRAINING COMPLETES)\n",
        "print(\"=== BERT FINE-TUNING RESULTS ===\")\n",
        "\n",
        "# Get final predictions\n",
        "predictions = trainer.predict(test_dataset)\n",
        "y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "# Convert back to label names\n",
        "y_test_labels = [id2label[label] for label in y_test]\n",
        "y_pred_labels = [id2label[pred] for pred in y_pred]\n",
        "\n",
        "# Print detailed results\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_labels, y_pred_labels))\n",
        "\n",
        "print(f\"\\nBERT F1 Score: {predictions.metrics['test_f1']:.3f}\")\n",
        "print(f\"BERT Accuracy: {predictions.metrics['test_accuracy']:.3f}\")\n",
        "print(f\"Target Achievement: {'‚úÖ EXCEEDED' if predictions.metrics['test_f1'] > 0.75 else '‚ö†Ô∏è BELOW TARGET'}\")"
      ],
      "metadata": {
        "id": "EqxC-jimH-Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with a new workplace narrative (DEVICE-CORRECTED)\n",
        "test_narrative = \"\"\"\n",
        "After I questioned the new remote work policy in a team meeting, my manager suddenly started requiring me to submit daily written reports about my activities. These reports were never required before and no one else has to do them. Every email I send now gets forwarded to HR with additional commentary about my communication style. Small issues like joining a meeting two minutes late are now documented in writing when they never were before.\n",
        "\"\"\"\n",
        "\n",
        "# Get prediction (fix device placement)\n",
        "device = next(model.parameters()).device  # Get model's device\n",
        "inputs = tokenizer(test_narrative, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "\n",
        "# Move inputs to same device as model\n",
        "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "    predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "    confidence = torch.max(predictions).item()\n",
        "\n",
        "print(\"=== REAL USER TEST ===\")\n",
        "print(f\"Input: {test_narrative[:100]}...\")\n",
        "print(f\"Predicted Pattern: {id2label[predicted_class]}\")\n",
        "print(f\"Confidence: {confidence:.3f}\")\n",
        "print(f\"All Probabilities:\")\n",
        "for i, prob in enumerate(predictions[0]):\n",
        "    print(f\"  {id2label[i]}: {prob:.3f}\")"
      ],
      "metadata": {
        "id": "hx2f2n14cJnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test multiple workplace scenarios with confidence analysis\n",
        "test_scenarios = [\n",
        "    {\n",
        "        \"text\": \"Placed on formal improvement plan after questioning budget decisions. Goals are vague and timeline unrealistic.\",\n",
        "        \"expected\": \"pip_tactics\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Manager gives different instructions in meetings versus private conversations. Won't clarify priorities in writing.\",\n",
        "        \"expected\": \"strategic_ambiguity\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Removed from team meetings without explanation. Colleagues avoid sharing project information with me.\",\n",
        "        \"expected\": \"isolation_tactics\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Every conversation now requires written follow-up. Minor issues become formal policy violations.\",\n",
        "        \"expected\": \"documentation_building\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Work has been stressful lately. Manager seems busy. Not sure what's happening with the project.\",\n",
        "        \"expected\": \"unclear/ambiguous\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"=== CONFIDENCE THRESHOLD ANALYSIS ===\")\n",
        "device = next(model.parameters()).device\n",
        "\n",
        "for i, scenario in enumerate(test_scenarios):\n",
        "    # Tokenize and predict\n",
        "    inputs = tokenizer(scenario[\"text\"], return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "        confidence = torch.max(predictions).item()\n",
        "\n",
        "    print(f\"\\n--- Scenario {i+1} ---\")\n",
        "    print(f\"Text: {scenario['text'][:80]}...\")\n",
        "    print(f\"Expected: {scenario['expected']}\")\n",
        "    print(f\"Predicted: {id2label[predicted_class]}\")\n",
        "    print(f\"Confidence: {confidence:.3f}\")\n",
        "\n",
        "    # Flag low confidence predictions\n",
        "    if confidence < 0.6:\n",
        "        print(\"‚ö†Ô∏è LOW CONFIDENCE - Human review recommended\")\n",
        "    elif confidence > 0.9:\n",
        "        print(\"‚úÖ HIGH CONFIDENCE - Reliable prediction\")\n",
        "    else:\n",
        "        print(\"üîÑ MEDIUM CONFIDENCE - Consider additional context\")"
      ],
      "metadata": {
        "id": "wmeOiodadOte"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}